{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WHxmK8UT_Cy"
      },
      "source": [
        "# Language generation and language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOqJsXzjT_Cz"
      },
      "source": [
        "Can you give the next word in the following phrases?\n",
        "\n",
        "> Never gonna give you _\n",
        ">\n",
        "> That's one small step for a man, one _\n",
        ">\n",
        "> A bird in the hand is worth _\n",
        ">\n",
        "> London bridge is falling _\n",
        ">\n",
        "> To be or not to be, that is _\n",
        ">\n",
        "> It was the best of times, it was _\n",
        ">\n",
        "> The quick brown fox jumped _\n",
        "\n",
        "The chances are, you were able to give the next word, if not complete the whole phrase. This is because, in a lot of cases, language use is stereotyped. Certain words follow from certain phrases, and we can use that to help a machine work fluently with language.\n",
        "\n",
        "We can use this facility in a few ways. One is to help us understand human speech input. Often, the sounds we make while speaking can have more that one interpretation (try saying quickly \"recognise speech\" and \"wreck a nice beach\" to someone, and ask them which is which). In these cases, having some idea of the likely words can help us disambiguate the sounds the machine hears. We can also use the predictability of language to detect spelling and grammar mistakes; a grammar checker can detect incongurous words and suggest them for revision.\n",
        "\n",
        "Another application, one we'll be looking at here, is about _generating_ text that reads like a plausible new example of some source. If we build a language model using only text from one source (or a limited range of sources), that model will reflect that corpus of text. If we generate text with that model, it should have a similar style to the source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dT7h7cPT_C0"
      },
      "source": [
        "## A language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQAQBheRT_C1"
      },
      "source": [
        "These applications rely on having a _language model_, a description of what the language should look like. There are many types of language model. You've probably heard of the \"large language models\" used by tools such as ChatGPT. For this example, we'll use a much smaller model, but it's surprising how well even this small, simple model can work.\n",
        "\n",
        "Our language model is inspired by the quiz above: if we know the last few words that have been used, we can make a prediction about what comes next. This is called an **_n_-gram** model in the literature, where _n_ is now many words of context we're using.\n",
        "\n",
        "For instance, let's we take the first line of _A Tale of Two Cities_\n",
        "\n",
        "> It was the best of times, it was the worst of times …\n",
        "\n",
        "We can build a 2-gram (bigram) model of this text. We slide a two-word-long window along the text and record, for each bigram, the word that comes next. Sliding the window looks a bit like this:\n",
        "\n",
        "| it | was | the | best | of | times | it | was | the | worst | of | times |\n",
        "|----|-----|-----|------|----|-------|----|-----|-----|-------|----|-------|\n",
        "| it | was | the |  |  |  |  |  |  |  |  |  |\n",
        "|  | was | the | best |  |  |  |  |  |  |  |  |\n",
        "|  |  | the | best | of |  |  |  |  |  |  |  |\n",
        "|  |  |  | best | of | times |  |  |  |  |  |  |\n",
        "|  |  |  |  | of | times | it |  |  |  |  |  |\n",
        "|  |  |  |  |  | times | it | was |  |  |  |  |\n",
        "|  |  |  |  |  |  | it | was | the |  |  |  |\n",
        "|  |  |  |  |  |  |  | was | the | worst |  |  |\n",
        "|  |  |  |  |  |  |  |  | the | worst | of |  |\n",
        "|  |  |  |  |  |  |  |  |  | worst | of | times |\n",
        "\n",
        "We can see that the bigram \"it was\" occurs twice in that sentence, and both times it is followed by the word \"the\". We can also see that the bigram \"was the\" occurs twice, but it is followed by different words each time: once by \"best\", once by \"worst\".\n",
        "\n",
        "The full bigram model from this sentence looks like this:\n",
        "\n",
        "* it was → the: 2\n",
        "* was the → best: 1, worst: 1\n",
        "* the best → of: 1\n",
        "* best of → times: 1\n",
        "* of times → it: 1, None: 1\n",
        "* times it → was: 1\n",
        "* the worst → of: 1\n",
        "* worst of → times: 1\n",
        "\n",
        "With only this short amount of text, the language model doesn't really tell us much interesting. We need more text. If we take the entire first chapter of the book, we find 878 bigrams, most of which occur only a couple of times. The most frequent ones are:\n",
        "\n",
        "* and a → knife: 1, queen: 2, thousand: 1\n",
        "* and seventy → five: 3\n",
        "* and the → fair: 1, farmer: 1, guard: 1, majesty: 1, mob: 1, musketeers: 1\n",
        "* by the → dozen: 1, other: 1, woodman: 1\n",
        "* hundred and → seventy: 3\n",
        "* in the → capital: 1, dark: 1, earthly: 1, hand: 1, life: 1, light: 1, midst: 1, rain: 1, rough: 1, superlative: 1, woods: 1\n",
        "* it was → clearer: 1, the: 11\n",
        "* of the → captain: 1, chickens: 1, cock: 1, common: 1, failure: 1, heavy: 1, large: 1, law: 1, plain: 1, revolution: 1, shield: 1, state: 1\n",
        "* on the → mob: 1, musketeers: 1, throne: 2, whole: 1\n",
        "* one thousand → seven: 3\n",
        "* seven hundred → and: 3\n",
        "* seventy five → conduct: 1, environed: 1, spiritual: 1\n",
        "* there were → a: 2, growing: 1, sheltered: 1\n",
        "* thousand seven → hundred: 3\n",
        "* to the → english: 1, human: 1, lords: 1\n",
        "* was the → age: 2, best: 1, epoch: 2, season: 2, spring: 1, winter: 1, worst: 1, year: 1\n",
        "* with a → fair: 1, high: 1, large: 2, plain: 1, sack: 1\n",
        "\n",
        "You can begin to see the flavour of Dickens in this model. For instance, the bigram \"was the\" shows that, at least in this chapter, Dickens was concerned with time and seasons.\n",
        "\n",
        "You should be able to see how we can use this model to generate text. If we're generating text, and we've just generated a particular _n_-gram, we can look up that _n_-gram in the language model and see the words that could come after it. We pick one of the listed words, weighted by the probability of occurrence, and emit that word. That gives us a new _n_-gram, and the process repeats.\n",
        "\n",
        "For instance, let's say we start with the bigram \"it was\". We can look up words that come next, and the most most likey is \"the\". We emit that word and update the \"most recent bigram\" to be \"was the\". We pick one words that could follow: \"season\". Next comes \"of\", then a choice between \"Light\" and \"Darkness\". We can build up more text as we want by repeating the process.\n",
        "\n",
        "\n",
        "| Emitted text | Current bigram | Word choices | Chosen next word |\n",
        "|--------------|----------------|--------------|------------------|\n",
        "| it was | it was | clearer: 1, the: 9 | the |\n",
        "| it was the | was the | age: 2, best: 1, epoch: 2, season: 2, spring: 1, winter: 1, worst: 1, year: 1 | season |\n",
        "| it was the season | the season | of: 2 | of |\n",
        "| it was the season of | season of | Darkness: 1, Light: 1 | Darkness |\n",
        "| it was the season of Darkness | of Darkness | it: 1 | it |\n",
        "| it was the season of Darkness it | Darkness it | was: 1 | was |\n",
        "| it was the season of Darkness it was | it was | clearer: 1, the: 9 | the |\n",
        "| it was the season of Darkness it was the | was the | age: 2, best: 1, epoch: 2, season: 2, spring: 1, winter: 1, worst: 1, year: 1 | age |\n",
        "| it was the season of Darkness it was the age | the age | of: 2 | of |\n",
        "| it was the season of Darkness it was the age of | age of | foolishness: 1, wisdom: 1 | wisdom |\n",
        "| it was the season of Darkness it was the age of wisdom | of wisdom | it: 1 | it |\n",
        "| it was the season of Darkness it was the age of wisdom it | wisdom it | was: 1 | was |\n",
        "\n",
        "Now you have the idea of how the _n_-gram language model is built and used, it's time to implement it. This has three stages.\n",
        "\n",
        "1. Represent the language model\n",
        "2. Read some text and populate the model\n",
        "3. Use the model to generate new text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82MYxEveT_C1"
      },
      "source": [
        "# Aside: reading text files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA6rgIcvT_C2"
      },
      "source": [
        "We need to read large amount of text to populate our language models: a novel's-worth is about the minimum we can get away with. We need to split that text into words (and punctuation). The reading and pre-processing this text is full of fiddly details that aren't worth going into. Instead, we'll just use these couple of functions to do the pre-processing for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K8JlKGATT_C2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import collections\n",
        "import unicodedata\n",
        "import random\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rCfmRA9NT_C3",
        "outputId": "178a9aaa-b1cc-45af-ead3-416ee6f914c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-28 15:34:31--  https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/le-mort-d-arthur.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 833972 (814K) [text/plain]\n",
            "Saving to: ‘le-mort-d-arthur.txt’\n",
            "\n",
            "le-mort-d-arthur.tx 100%[===================>] 814.43K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-28 15:34:31 (13.9 MB/s) - ‘le-mort-d-arthur.txt’ saved [833972/833972]\n",
            "\n",
            "--2025-08-28 15:34:31--  https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/odyssey.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 621609 (607K) [text/plain]\n",
            "Saving to: ‘odyssey.txt’\n",
            "\n",
            "odyssey.txt         100%[===================>] 607.04K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-08-28 15:34:32 (11.2 MB/s) - ‘odyssey.txt’ saved [621609/621609]\n",
            "\n",
            "--2025-08-28 15:34:32--  https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/pride-and-prejudice.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 699449 (683K) [text/plain]\n",
            "Saving to: ‘pride-and-prejudice.txt’\n",
            "\n",
            "pride-and-prejudice 100%[===================>] 683.06K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-08-28 15:34:32 (12.2 MB/s) - ‘pride-and-prejudice.txt’ saved [699449/699449]\n",
            "\n",
            "--2025-08-28 15:34:32--  https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/tale-of-two-cities.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 757021 (739K) [text/plain]\n",
            "Saving to: ‘tale-of-two-cities.txt’\n",
            "\n",
            "tale-of-two-cities. 100%[===================>] 739.28K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-28 15:34:32 (13.1 MB/s) - ‘tale-of-two-cities.txt’ saved [757021/757021]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/le-mort-d-arthur.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/odyssey.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/pride-and-prejudice.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/NeilNjae/nominet-quick-start-ai/main/2.markov-chains/tale-of-two-cities.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ppNO75tXT_C3",
        "outputId": "88221637-73ca-4a14-de4d-702579c43ca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipython-input-3273687067.py:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  punctuation_pattern = re.compile('(\\d+\\.\\d+|\\w+\\'\\w+|[{0}]+(?=\\w)|(?<=\\w)[{0}]+|[{0}]+$)'.format(re.escape(string.punctuation)))\n"
          ]
        }
      ],
      "source": [
        "token_pattern = re.compile(r'[^{}]+'.format(re.escape(string.ascii_letters + string.digits + string.punctuation)))\n",
        "punctuation_pattern = re.compile('(\\d+\\.\\d+|\\w+\\'\\w+|[{0}]+(?=\\w)|(?<=\\w)[{0}]+|[{0}]+$)'.format(re.escape(string.punctuation)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rE5s1dJjT_C3"
      },
      "outputs": [],
      "source": [
        "def tokenise(text):\n",
        "    \"\"\"Split a text string into tokens, splitting on spaces and punctuation,\n",
        "    but keeping multiple punctuation characters as one token.\"\"\"\n",
        "    return [ch for gp in [re.split(punctuation_pattern, t) for t in re.split(token_pattern, text)]\n",
        "        for ch in gp if ch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Abf1I0yBT_C3"
      },
      "outputs": [],
      "source": [
        "def sjoin(tokens):\n",
        "    \"\"\"Combine a set of tokens into a string for pretty-printing.\"\"\"\n",
        "    sentence = ''\n",
        "    for t in tokens:\n",
        "        if t[-1] not in \".,:;')-!?\":\n",
        "            sentence += ' '\n",
        "        sentence += t\n",
        "    return sentence.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KAuq9kWqT_C3",
        "outputId": "b709966e-bd29-4f0e-bfd3-d99e5b7aec63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'cat',\n",
              " 'sat',\n",
              " 'on',\n",
              " 'the',\n",
              " 'mat',\n",
              " '.',\n",
              " 'The',\n",
              " 'quick',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'jumped',\n",
              " 'over',\n",
              " 'the',\n",
              " 'lazy',\n",
              " 'dog',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "sample_text = 'The cat sat on the mat. The quick brown fox jumped over the lazy dog.'\n",
        "tokenise(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7rmDc5fqT_C3",
        "outputId": "5dd7d232-545c-4285-ed2b-83297c750448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The cat sat on the mat. The quick brown fox jumped over the lazy dog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sjoin(tokenise(sample_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eCLCy7iT_C4"
      },
      "source": [
        "## Representing the language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjP1pYGHT_C4"
      },
      "source": [
        "Now we understand what the language model should look like, we can work out how to represent it in Python.\n",
        "\n",
        "The language model is a two-layered data structure. We have a bunch of _n_-grams; for each _n_-gram, we have a bunch of word choices; for each word choice we have a frequency of occurrence. These are key-value stores, so Python `dict`s are the obvious choice. That gives us a structure that looks like this:\n",
        "\n",
        "**Language model**\n",
        "| Key | Value |\n",
        "|-----|-------|\n",
        "| _n_-gram | word choices |\n",
        "\n",
        "**Word choices**\n",
        "| Key | Value |\n",
        "|-----|-------|\n",
        "| word | frequency |\n",
        "\n",
        "However, Python provides a couple of variations on `dict`s, in the [`collections`](https://docs.python.org/3/library/collections.html) library, that will make our lives easier.\n",
        "\n",
        "The first is a `Counter`, a `dict` specialised for counting things. We'll use this for counting the frequency of words. If we pass a sequence of things to a `Counter`, we get the counts of how often each thing occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SDxyJfseT_C4",
        "outputId": "0ae47b4b-18c6-44bd-cd62-284f1fb525ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "counts = collections.Counter(tokenise(\"the cat sat on the mat\"))\n",
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekVKW-ckT_C4"
      },
      "source": [
        "If we ask about a thing, we're told how often it occurs. Unknown keys don't generate an error, but return a count of zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YFIWjLnIT_C4",
        "outputId": "727158b6-6e45-4ba6-f347-8350ac228192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "counts['the'], counts['aardvark']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2n8LtS3T_C4"
      },
      "source": [
        "If we want to count more things, we use the `update` method and pass in the new things to be counted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ena8n_LcT_C4",
        "outputId": "c42ab173-f104-465f-e1d2-b70605d885f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'the': 3,\n",
              "         'cat': 1,\n",
              "         'sat': 1,\n",
              "         'on': 1,\n",
              "         'mat': 1,\n",
              "         'quick': 1,\n",
              "         'brown': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "counts.update(['the', 'quick', 'brown'])\n",
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUfRuCP1T_C4"
      },
      "source": [
        "The other useful `dict` variant is a `defaultdict`. This behaves exactly like a normal `dict` except it gives a default value if we ask for a missing key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0rwKiQWZT_C4",
        "outputId": "57560820-d549-4ffb-bffd-3f987ef9e292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(str, {3: 'hat', 6: 'banana'})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dd = collections.defaultdict(str)\n",
        "dd[3] = 'hat'\n",
        "dd[6] = 'banana'\n",
        "dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RXVt8JqgT_C5",
        "outputId": "c57ce112-e566-4ed7-a02a-cf88f9803928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hat', '')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dd[3], dd[99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i1eeuvKT_C5"
      },
      "source": [
        "Using a `defaultdict` means we don't have to check an element exists before we update it.\n",
        "\n",
        "The other wrinkle is that Python won't let us use (mutable) lists of words as the keys to a `dict`-like structure, so we have to convert each _n_-grams from a `list` to a `tuple`.\n",
        "\n",
        "But with all those implementation details out of the way, let's get on with some programming!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qfD45RZT_C5"
      },
      "source": [
        "# Buidling the language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENnpcMP6T_C5"
      },
      "source": [
        "With the utilities above, we can read a text file and split it into tokens. Our next job is to use that stream of tokens to build the language model.\n",
        "\n",
        "We'll build this up in stages, working from finding _n_-grams in a list to building the whole language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "SvRHNIVMT_C5"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiapNX9FT_C5"
      },
      "source": [
        "Write a piece of code that will find and print the trigrams (three-word slices) of `tokenise(sample_text)`. The last couple could well be shorter than three words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PazphuseT_C5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5IowrWFT_C5"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3lSoWCUXT_C5",
        "outputId": "e7f5e427-0088-4a75-e2d9-102e77fd5131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cat', 'sat']\n",
            "['cat', 'sat', 'on']\n",
            "['sat', 'on', 'the']\n",
            "['on', 'the', 'mat']\n",
            "['the', 'mat', '.']\n",
            "['mat', '.', 'The']\n",
            "['.', 'The', 'quick']\n",
            "['The', 'quick', 'brown']\n",
            "['quick', 'brown', 'fox']\n",
            "['brown', 'fox', 'jumped']\n",
            "['fox', 'jumped', 'over']\n",
            "['jumped', 'over', 'the']\n",
            "['over', 'the', 'lazy']\n",
            "['the', 'lazy', 'dog']\n",
            "['lazy', 'dog', '.']\n",
            "['dog', '.']\n",
            "['.']\n"
          ]
        }
      ],
      "source": [
        "sentence = tokenise(sample_text)\n",
        "for i in range(len(sentence)):\n",
        "    print(sentence[i:i+3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "eOIML5FkT_C5"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbuEtKzAT_C5"
      },
      "source": [
        "Modify that code so it doesn't generate the final too-short trigram. This will mean stopping the loop earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RE4WzqET_C5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjuRatGET_C5"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BrHUXnBwT_C6",
        "outputId": "d62098bf-7dc3-4a80-ba33-93c30746f98f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cat', 'sat']\n",
            "['cat', 'sat', 'on']\n",
            "['sat', 'on', 'the']\n",
            "['on', 'the', 'mat']\n",
            "['the', 'mat', '.']\n",
            "['mat', '.', 'The']\n",
            "['.', 'The', 'quick']\n",
            "['The', 'quick', 'brown']\n",
            "['quick', 'brown', 'fox']\n",
            "['brown', 'fox', 'jumped']\n",
            "['fox', 'jumped', 'over']\n",
            "['jumped', 'over', 'the']\n",
            "['over', 'the', 'lazy']\n",
            "['the', 'lazy', 'dog']\n",
            "['lazy', 'dog', '.']\n"
          ]
        }
      ],
      "source": [
        "sentence = tokenise(sample_text)\n",
        "for i in range(len(sentence)-2):\n",
        "    print(sentence[i:i+3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "hw2KDlbNT_C6"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypy_6W5PT_C6"
      },
      "source": [
        "Modify the code above to find the bigram we want and the next token. Convert the bigram from a list to a tuple and store it in a variable `ngram`. Store the next token in a variable `next_token`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb4AXCuWT_C6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDDq5j8cT_C-"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5VADC31DT_C-",
        "outputId": "66fc930b-188e-4b8b-c3cd-a0fa35c81b27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The', 'cat') sat\n",
            "('cat', 'sat') on\n",
            "('sat', 'on') the\n",
            "('on', 'the') mat\n",
            "('the', 'mat') .\n",
            "('mat', '.') The\n",
            "('.', 'The') quick\n",
            "('The', 'quick') brown\n",
            "('quick', 'brown') fox\n",
            "('brown', 'fox') jumped\n",
            "('fox', 'jumped') over\n",
            "('jumped', 'over') the\n",
            "('over', 'the') lazy\n",
            "('the', 'lazy') dog\n",
            "('lazy', 'dog') .\n"
          ]
        }
      ],
      "source": [
        "sentence = tokenise(sample_text)\n",
        "for i in range(len(sentence)-2):\n",
        "    ngram = tuple(sentence[i:i+2])\n",
        "    next_token = sentence[i+2]\n",
        "    print(ngram, next_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "S_kn1OX4T_C-"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Y-ZBCfT_C_"
      },
      "source": [
        "Now store these bigrams and next tokens in a language model.\n",
        "\n",
        "You can create an empty language model with the line\n",
        "\n",
        "```python\n",
        "model = collections.defaultdict(collections.Counter)\n",
        "```\n",
        "\n",
        "You can push these results into our language model, with the line\n",
        "\n",
        "```python\n",
        "model[ngram].update([next_token])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmdv4f9oT_C_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwUjS4KwT_C_"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZWLZsEIAT_C_",
        "outputId": "83265b02-563f-4956-a79e-8889dc93f513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(collections.Counter,\n",
              "            {('The', 'cat'): Counter({'sat': 1}),\n",
              "             ('cat', 'sat'): Counter({'on': 1}),\n",
              "             ('sat', 'on'): Counter({'the': 1}),\n",
              "             ('on', 'the'): Counter({'mat': 1}),\n",
              "             ('the', 'mat'): Counter({'.': 1}),\n",
              "             ('mat', '.'): Counter({'The': 1}),\n",
              "             ('.', 'The'): Counter({'quick': 1}),\n",
              "             ('The', 'quick'): Counter({'brown': 1}),\n",
              "             ('quick', 'brown'): Counter({'fox': 1}),\n",
              "             ('brown', 'fox'): Counter({'jumped': 1}),\n",
              "             ('fox', 'jumped'): Counter({'over': 1}),\n",
              "             ('jumped', 'over'): Counter({'the': 1}),\n",
              "             ('over', 'the'): Counter({'lazy': 1}),\n",
              "             ('the', 'lazy'): Counter({'dog': 1}),\n",
              "             ('lazy', 'dog'): Counter({'.': 1})})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sentence = tokenise(sample_text)\n",
        "model = collections.defaultdict(collections.Counter)\n",
        "for i in range(len(sentence)-2):\n",
        "    ngram = tuple(sentence[i:i+2])\n",
        "    next_token = sentence[i+2]\n",
        "    model[ngram].update([next_token])\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm-Snxd5T_C_"
      },
      "source": [
        "The final step is to take the code we've written and wrap it in a function definition, to make it easy to call for each sentence we process.\n",
        "\n",
        "While we're at it, we get rid of the \"magic number\" 2 in the code, and replace it with a parameter for the tuple size.\n",
        "\n",
        "That gives us the function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SIVHWrNNT_C_"
      },
      "outputs": [],
      "source": [
        "def build_model(text, tuple_size=2):\n",
        "    model = collections.defaultdict(collections.Counter)\n",
        "    # Record each n-gram in turn\n",
        "    for i in range(len(text) - tuple_size):\n",
        "        n_gram = text[i:i+tuple_size]\n",
        "        next_word = text[i+tuple_size]\n",
        "        model[tuple(n_gram)].update([next_word])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "LpaAy9tjT_C_",
        "outputId": "2c409ea4-e13c-4140-905e-a4d01f412990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(collections.Counter,\n",
              "            {('The', 'cat'): Counter({'sat': 1}),\n",
              "             ('cat', 'sat'): Counter({'on': 1}),\n",
              "             ('sat', 'on'): Counter({'the': 1}),\n",
              "             ('on', 'the'): Counter({'mat': 1}),\n",
              "             ('the', 'mat'): Counter({'.': 1}),\n",
              "             ('mat', '.'): Counter({'The': 1}),\n",
              "             ('.', 'The'): Counter({'quick': 1}),\n",
              "             ('The', 'quick'): Counter({'brown': 1}),\n",
              "             ('quick', 'brown'): Counter({'fox': 1}),\n",
              "             ('brown', 'fox'): Counter({'jumped': 1}),\n",
              "             ('fox', 'jumped'): Counter({'over': 1}),\n",
              "             ('jumped', 'over'): Counter({'the': 1}),\n",
              "             ('over', 'the'): Counter({'lazy': 1}),\n",
              "             ('the', 'lazy'): Counter({'dog': 1}),\n",
              "             ('lazy', 'dog'): Counter({'.': 1})})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "sample_model = build_model(tokenise(sample_text))\n",
        "sample_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TxXZ43moT_C_",
        "outputId": "6325ab82-cccd-4e0c-cd29-cec4dd3bbd78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "967"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "sample_chapter = open('tale-of-two-cities.txt').read()[1882:7653]\n",
        "sample_chapter_model = build_model(tokenise(sample_chapter.lower()), tuple_size=2)\n",
        "len(sample_chapter_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XjKirXjpT_DA",
        "outputId": "0c3d36cd-8b13-4d5b-d0e4-c59346f35c96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'the': 11, 'clearer': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "sample_chapter_model[('it', 'was')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kVdGxxhT_DA"
      },
      "source": [
        "# Generating text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K5EOUK8T_DA"
      },
      "source": [
        "Now we have a model, we can use it to generate text.\n",
        "\n",
        "The process is roughly the reverse of how we created the language model. We have a current _n_-gram. We find that _n_-gram in the language model, look up the possible next tokens, and pick one. We update the current _n_-gram to include this next token, and the process repeats. Meanwhile, we keep track of all the generated tokens.\n",
        "\n",
        "A typical generation run is below. You can see how the language model guides the generation of the text.\n",
        "\n",
        "| Generated text | Current _n_-gram | Next token options | Chosen token |\n",
        "|----------------|------------------|--------------------|--------------|\n",
        "| it was | it was | the → 11, clearer → 1 | the |\n",
        "| it was the | was the | age → 2, epoch → 2, season → 2, best → 1, worst → 1, spring → 1, winter → 1, year → 1 | season |\n",
        "| it was the season | the season | of → 2 | of |\n",
        "| it was the season of | season of | light → 1, darkness → 1 | darkness |\n",
        "| it was the season of darkness | of darkness | , → 1 | , |\n",
        "| it was the season of darkness , | darkness , | it → 1 | it |\n",
        "| it was the season of darkness , it | , it | was → 9 | was |\n",
        "| it was the season of darkness , it was | it was | the → 11, clearer → 1 | the |\n",
        "| it was the season of darkness , it was the | was the | age → 2, epoch → 2, season → 2, best → 1, worst → 1, spring → 1, winter → 1, year → 1 | age |\n",
        "| it was the season of darkness , it was the age | the age | of → 2 | of |\n",
        "| it was the season of darkness , it was the age of | age of | wisdom → 1, foolishness → 1 | foolishness |\n",
        "| it was the season of darkness , it was the age of foolishness | of foolishness | , → 1 | , |\n",
        "| it was the season of darkness , it was the age of foolishness , | foolishness , | it → 1 | it |\n",
        "| it was the season of darkness , it was the age of foolishness , it | , it | was → 9 | was |\n",
        "| it was the season of darkness , it was the age of foolishness , it was | it was | the → 11, clearer → 1 | the |\n",
        "| it was the season of darkness , it was the age of foolishness , it was the | was the | age → 2, epoch → 2, season → 2, best → 1, worst → 1, spring → 1, winter → 1, year → 1 | year |\n",
        "| it was the season of darkness , it was the age of foolishness , it was the year | the year | of → 1, one → 1 | of |\n",
        "| it was the season of darkness , it was the age of foolishness , it was the year of | year of | our → 1 | our |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwOPFw0AT_DA"
      },
      "source": [
        "## Picking a random next token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLvoFvHST_DA"
      },
      "source": [
        "One thing we need to do is pick a suitable next token, given a particular _n_-gram and a langauge model.\n",
        "\n",
        "Python's built-in `random` library has a function `choice()` that will select a random element from a list.\n",
        "\n",
        "The `Counter` object has a method `elements()` that will return all the items in the `Counter`, each appearing as many times as its count. `elements()` returns an _iterator_, so we need to wrap it in a call to `list` to convert it to the list that `choice` needs.\n",
        "\n",
        "This cell generates all possible next words for a given _n_-gram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "efdIRBSVT_DA",
        "outputId": "6816e40b-8436-4a59-c21d-68b83a6239f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best',\n",
              " 'worst',\n",
              " 'age',\n",
              " 'age',\n",
              " 'epoch',\n",
              " 'epoch',\n",
              " 'season',\n",
              " 'season',\n",
              " 'spring',\n",
              " 'winter',\n",
              " 'year']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "list(sample_chapter_model[('was', 'the')].elements())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzNo85EcT_DA"
      },
      "source": [
        "This cell picks one of them at random. If you run this cell a few times, you should see different results most of the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5b3L4UEmT_DA",
        "outputId": "0be7e1c4-0ff5-4237-bc7f-23434ded5812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'winter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "random.choice(list(sample_chapter_model[('was', 'the')].elements()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bVDbjTcT_DA"
      },
      "source": [
        "(There are other choices for how to select the next item, but we won't go into that here.)\n",
        "\n",
        "This is the procedure that will generate text for us. The body of it is the `while` loop, that generates a new token while the current _n_-gram exists in the language model, and the generated text isn't longer than the limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CF10s_qST_DB"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, starting_ngram=None, max_length=500):\n",
        "    if starting_ngram:\n",
        "        current = starting_ngram\n",
        "    else:\n",
        "        current = random.choice(list(model))\n",
        "    generated = list(current)\n",
        "    while current in model and len(generated) < max_length:\n",
        "        next_item = random.choice(list(model[current].elements()))\n",
        "        # print(generated, ':', current, ':', model[current], ':', next_item)\n",
        "        generated.append(next_item)\n",
        "        current = current[1:] + (next_item, )\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVWtcnHwT_DB"
      },
      "source": [
        "We can test this with the `sample_model` created above. This will test the procedure runs without errors, but doesn't produce exciting text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "EMxlztVET_DB",
        "outputId": "b7cc87dc-c5f8-436c-e93e-a7ea76256252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'. The quick brown fox jumped over the lazy dog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "sjoin(generate_text(sample_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVOMgwLyT_DB"
      },
      "source": [
        "Next we load the first chapter of _A Tale of Two Cities\". For information, we show how many distinct bigrams are in this chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qZoBGbrKT_DB",
        "outputId": "149b9017-bf86-4554-d681-1085da4c1648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "967"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "sample_chapter = open('tale-of-two-cities.txt').read()[1882:7653]\n",
        "sample_chapter_model = build_model(tokenise(sample_chapter.lower()), tuple_size=2)\n",
        "len(sample_chapter_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma1bsU8DT_DB"
      },
      "source": [
        "We can now generate som text, starting with the same opening phrase. We limit the output to 20 tokens, but you can increase it if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wZ3mRWkxT_DB",
        "outputId": "e9410da2-72c4-47d6-84f6-c82a752983b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it was the epoch of belief, it was the worst of times, it was the epoch of incredulity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "sjoin(generate_text(sample_chapter_model, starting_ngram=('it', 'was'), max_length=20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoQ0cDBtT_DB"
      },
      "source": [
        "With this seeming to work, let's load the whole book…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GDqR6tnbT_DB",
        "outputId": "7a83ec1b-26d9-4ca0-f1f2-7669cc9351c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128671"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "two_cities = open('tale-of-two-cities.txt').read()\n",
        "two_cities_model = build_model(tokenise(two_cities), tuple_size=3)\n",
        "len(two_cities_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KgtVJCcT_DC"
      },
      "source": [
        "…and generate some text. Run this cell several times, and you'll see different text generated each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KkGzXcpxT_DC",
        "outputId": "c4eabfc2-f03d-45a9-8506-c8d678557bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'him lounging in the window, and of destructive upheaving of wave against wave, whose depths were yet unfathomed and whose forces were yet unknown. The remorseless sea of turbulently swaying shapes, voices of vengeance, and faces are often turned up to some of them for a single moment released the other\\'s eyes. Madame Defarge, surely !\" said Mr Lorry. \" No, dear Doctor Manette. Is there no old banker, no old business, no old servant, no old servant, no old time, long dead of broken'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "sjoin(generate_text(two_cities_model,  max_length=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGoAgvpxT_DC"
      },
      "source": [
        "Just printing the text gives annoying breaks in the middle of words. If we produce HTML text, the browser will make it prettier for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YKNfujZ8T_DC"
      },
      "outputs": [],
      "source": [
        "def pprint(tokens):\n",
        "    display(HTML(sjoin(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "V9UpVDNpT_DC",
        "outputId": "74244f16-72af-4c28-9583-1d671b5a17ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ", got into his carriage, sufficiently disturbing the darkness to elicit loud remonstrance from an owl in the roof of the hearse as could by any exercise of ingenuity stick upon it. Among the first of these volunteers was Jerry Cruncher himself, who modestly concealed his spiky head down. \" There's all manner of gestures while he spoke, and he kept his place with her as she mused, and the night before that, and he looked into the room in which he walked. Thus, Saint Antoine in this vinous feature of"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(generate_text(two_cities_model,  max_length=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "fEQdL84gT_DC",
        "outputId": "9bd8874a-8ddc-4c69-8dde-fba8972b4d39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95956"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "odyssey = open('odyssey.txt').read()\n",
        "odyssey_model = build_model(tokenise(odyssey), tuple_size=3)\n",
        "len(odyssey_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8Ux7aFTjT_DC",
        "outputId": "a493bfb5-77ec-40eb-8a96-ea0b3fd8d325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109621"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "pride = open('pride-and-prejudice.txt').read()\n",
        "pride_model = build_model(tokenise(pride), tuple_size=3)\n",
        "len(pride_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "BLSECZ0eT_DC",
        "outputId": "dd1eb327-ced7-4aa8-dfa7-0cad107d263a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104656"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "arthur = open('le-mort-d-arthur.txt').read()\n",
        "arthur_model = build_model(tokenise(arthur), tuple_size=3)\n",
        "len(arthur_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmrHm0aGT_DC"
      },
      "source": [
        "## Generating random text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reSZxWBLT_DD"
      },
      "source": [
        "We've got all the parts. Let's generate some large pieces of text. Do these have the same style as the originals? Are those styles distinctive enough to tell which model generated which text?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ghWtDtXJT_DD",
        "outputId": "18e383f4-1d44-4d24-f70e-01d880e21e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "has been set for them in a thicket just as they were being driven into them. But never mind, I will not hurt you.' \" He got more and more furious as he heard the omen that the son of Saturn, King of kings, answer me this question What do you propose to do? Will you set them fighting still further, or will you wait here and let me make you immortal, no matter how dire is his distress. I am host, and the Erembians, and to make Penelope leave off crying.' Be so kind,' he said, and went on by land till he came to at the hands of Aegisthus and a fearful reckoning did Aegisthus presently pay. See what a good thing if she would take as good care of him as alive and on his way soon enough. Let us warm the bow and the pieces of iron before the suitors, who eat them up without shame or scruple; but the suitors kept looking at one another and laughed, while pretty Melantho began to gibe at him contemptuously. She was daughter to great King Creon, and married the redoubtable son of Amphitryon. \" I also saw fair Epicaste mother of king Oedipodes whose awful lot it was to grind wheat and barley, the trees are loaded with bread and meat; he was by no means the worst man there; you are not going to save a single one of them every other day throughout all time, and would load the pyre with good things. I will tell it you; give me, therefore, and in the meantime Ulysses was getting his dinner. Then she knew me at once, lest any of them should venture along with myself to lift it and bore it into the brute, but the suitors went within the precincts of the house. Eteoneus carved the meat and gave them each their portions, while Megapenthes poured out the wine. Then she went away, and Minerva heard his prayer and forthwith thundered high up among the clouds from the splendour of Olympus, and Ulysses had offered up the thigh bones [ on the embers ] in the name of Neptune, Telemachus and his crew from Pylos had reached the town of Troy in company with yourself. We know what fate befell each one of the bearing- posts with her maids as she sat in her own room and cry; whereon all the maids in the house, but now that his master was gone he was lying neglected on the heaps of mule and cow dung that lay in front of them and threw their spears. He saw the greatness of my house, and they take no account of their neighbours"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(generate_text(odyssey_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "w0ySLlw9T_DD",
        "outputId": "fd745dee-9c1c-4062-c690-a578942b74bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "and three boys, who had balances at Tellson's. Whether his meditations on mortality had touched his liver, or whether he desired to show a little attention to an eminent man, is not conducive to health. What is it ?\" asked Mr Cruncher. \" Speak well of the Republic? The populace cried enthusiastically, \" No !\" until they left off, of their own wills they will go far enough to find her! \" Bad Fortune !\" cries The Vengeance, calling after him. He worked, and worked on, half an hour after Mr Lorry could discover, was, to smash this witness like a crockery vessel, and shiver his part of the country. Who was he ?\" \" Your husband is not my name. I am going to make, and exact the promise I am going to play backgammon with you, at that very moment waiting for the completion of its load, had been hurried and incomplete. He knew very well, that in his horror of the deed which had culminated the bad deeds and bad reputation of the old rag she wound about her head again. No crowd was about the door; no people were discernible at any of the reckonings over which she presided. Madame Defarge looked superciliously at the client, and nodded in confirmation. \" As to the nature of the two cases to which I had been that day put to death, and that was a great crowd about it, in the live red coals. A bottle of good claret after dinner does a digger in the red coals no harm, otherwise than as it has a tendency to throw him out of work. Mr Lorry, gently laying his hand on the Doctor's daughter had known nothing of the country. Who was he ?\" \" French. You don't know him ,\" said Mr Lorry. He was otherwise quite self- possessed, and those of Stryver, which above all were coarse and galling, for old reasons. Upon those, had followed Gabelle's letter: the appeal of an innocent prisoner, in danger of turning in the direction of the sanctuary, \" to let in a little while, he sat down to write until such time as the prison lamps should be extinguished. He wrote a long letter to Lucie, and to come at it in the hard heart of either of them would harm you .\" \" Stop !\" For an instant, and no one now looked at him twice, without looking at her, and hurried her into the room from which he has spoken. I was conducted to this chamber straight, the cries growing louder as we ascended the stairs, and touching it up with eagerness"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(generate_text(two_cities_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fuoA4Y2cT_DD",
        "outputId": "b7686e1d-baf6-432a-f9a9-129ebdffb201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "his assertions; for, though feeling almost secure, and with reason, for Charlotte had been tolerably encouraging, he was an indolent man, who lived only to eat, drink, and play at cards, who, though good themselves, ( my father particularly, all that was affectionate and insincere. She wrote even to Jane on the occasion. Mrs Bennet was doubly engaged, on one of the benches, and prepared in the highest spirits for the conquest of all that had occurred to make my former interference in his affairs absurd and impertinent. His surprise was great. What Wickham had said of her father, drily; \" and I could wish, sir: but I dare say I shall soon see them here .\" Elizabeth made no answer. He seemed astonished too on finding her alone, and fearful of inquiries or hints from her uncle and aunt would have been a disgrace; when she saw how much affection and solicitude they showed for Jane. The apothecary came; and Mrs Collins's happiness; and if that were the case, and with a low bow he left her to attack Mr Darcy, that I had forfeited all claim to it by extravagance, imprudence, in short, anything or nothing. Certain it is that I condescended to adopt the measures of art so far as to courtesy and hold out her hand to both. CHAPTER XXXVIII. On Saturday morning Elizabeth and Mr Collins seemed neither in need of encouragement, nor inclined to be silent. Only let me assure you, to keep my engagement; and, after some contrivance, the whole party were in hopes of an answer: but his companion was not disposed to make them every possible amends; but of this hereafter. If you do not choose to understand me, forgive my impertinence. Your uncle is as much surprised as I am missed. I am happier even than Jane; she only smiles, I laugh. Mr Darcy had to decide on what is right than a young lady like yourself ;\" and with a proper air of indifference, he soon afterwards said,-- \" Miss Bennet, that you have actually seen Pemberley .\" She replied in the affirmative; and, however bare of news the country in general might be, they always contrived to learn some from their aunt. At present, indeed, I am determined. We want none of them; and they both sat down. He then handed her in, Maria followed, and the younger sisters not worth speaking to, a wish of inquiring. His countenance, voice, and manner, had established him at once in the possession of every virtue. She tried to recollect"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(generate_text(pride_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "e9eiQ_idT_DD",
        "outputId": "a26b6f1f-e9ad-4445-d0fd-f2b851d934e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "knight made no words, for or now we saw never no knight match the Red Knight of the Red Castle, and the knights were both stonied. And as the damosel will lead me to the worse. With this this knight rode his way, and at night they sent unto him that sent you that letter. Alas, said Balin, and when he came home, the lady and the better made. How now, said the king, and there made Palamides to bear no more harness in a year. Then Sir Feldenak thought to revenge the death of Lanceor, and dress you unto me, my brother? Yea, said the damosel unto him with his dinner, there came a damosel from a knight, as ye will have me. That is overmuch said, said Sir Palomides, and she is most liefest to me. Alas, she said, he is alive. And then they let run their horses with all their host put them to the court, for an ye said other I know the king well; he is nothing sib to him, and demanded of them where Sir Gareth was in the forest a- hunting; for that same day Galahad the haut prince came with King Arthur, and well be we met, and bethink thee now of the despite thou didst me of the sending of the horn unto King Mark's court rather than Arthur's court, and brought him to King Arthur, and the men that were then there gathered together, and there she found a well, and hath made him a duke and knight of the world I loved him most, and had marvel of him. Sir, said Sir Gilmere, and bade him turn, and the King of Brittany, he hath slain two knights of his blood, for so it rehearseth in the book of venery, and beasts of chase, and slew the five kings, that went from him when that he wounded Sir Belleus at the pavilion. And that day Sir Tristram met with two knights. Thus endeth this tale of Sir Launcelot du Lake. As for that, it may not be answered, for the honour of knighthood, and therefore I dread you the more; and at the last smote off his head; wherefore I pray you let him ride after me; but until that time I will not make her wroth, I will be ready at all times. And then Sir Tristram bade him turn and give again the child. CHAPTER XXI. How King Arthur held in Wales, at a Pentecost, a great feast with mirth and joy, so soon after he returned unto Sir Plenorius, and there Sir"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(generate_text(arthur_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU9BLL9AT_DD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWlDmx-0T_DD"
      },
      "source": [
        "# Merging models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Uwgs7cT_DD"
      },
      "source": [
        "We have several language models. As you've seen, each generates text in the style of the source text. What happens if we combine models?\n",
        "\n",
        "The mechanics of this are fairly easy. The `Counter`s we're using can be added with the `+` operator. This does what you'd expect, and adds all the counts of the two `Counter`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "EcGBMf-2T_DD"
      },
      "outputs": [],
      "source": [
        "c1 = collections.Counter(tokenise(\"the cat sat on the mat\"))\n",
        "c2 = collections.Counter(tokenise(\"the cat lay on the bed\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbySm42_T_DD"
      },
      "outputs": [],
      "source": [
        "c1, c2, c1 + c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "xJVXJEypT_DE"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGqSls6GT_DE"
      },
      "source": [
        "Given the two models below, create a **new** model that combines them. The result should be:\n",
        "\n",
        "```python\n",
        "defaultdict(collections.Counter,\n",
        "            {('the', 'cat'): Counter({'sat': 1, 'lay': 1}),\n",
        "             ('cat', 'sat'): Counter({'on': 1}),\n",
        "             ('sat', 'on'): Counter({'the': 1}),\n",
        "             ('on', 'the'): Counter({'mat': 1, 'bed': 1}),\n",
        "             ('cat', 'lay'): Counter({'on': 1}),\n",
        "             ('lay', 'on'): Counter({'the': 1})})\n",
        "```\n",
        "\n",
        "(The order of elements in the model may vary, but the contents should be the same.)\n",
        "\n",
        "Ensure that both source models remain unchanged by the merge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kLnrpbfiT_DE",
        "outputId": "9432e607-8387-4b18-a73b-5c8a0451b25a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(defaultdict(collections.Counter,\n",
              "             {('the', 'cat'): Counter({'sat': 1}),\n",
              "              ('cat', 'sat'): Counter({'on': 1}),\n",
              "              ('sat', 'on'): Counter({'the': 1}),\n",
              "              ('on', 'the'): Counter({'mat': 1})}),\n",
              " defaultdict(collections.Counter,\n",
              "             {('the', 'cat'): Counter({'lay': 1}),\n",
              "              ('cat', 'lay'): Counter({'on': 1}),\n",
              "              ('lay', 'on'): Counter({'the': 1}),\n",
              "              ('on', 'the'): Counter({'bed': 1})}))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "m1 = build_model(tokenise(\"the cat sat on the mat\"))\n",
        "m2 = build_model(tokenise(\"the cat lay on the bed\"))\n",
        "m1, m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V20QExUHT_DE"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "en7TA1QST_DE",
        "outputId": "64511706-c3f5-4d5e-e5eb-28c93099c8f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(defaultdict(collections.Counter,\n",
              "             {('the', 'cat'): Counter({'sat': 1, 'lay': 1}),\n",
              "              ('cat', 'sat'): Counter({'on': 1}),\n",
              "              ('sat', 'on'): Counter({'the': 1}),\n",
              "              ('on', 'the'): Counter({'mat': 1, 'bed': 1}),\n",
              "              ('cat', 'lay'): Counter({'on': 1}),\n",
              "              ('lay', 'on'): Counter({'the': 1})}),\n",
              " defaultdict(collections.Counter,\n",
              "             {('the', 'cat'): Counter({'sat': 1}),\n",
              "              ('cat', 'sat'): Counter({'on': 1}),\n",
              "              ('sat', 'on'): Counter({'the': 1}),\n",
              "              ('on', 'the'): Counter({'mat': 1})}),\n",
              " defaultdict(collections.Counter,\n",
              "             {('the', 'cat'): Counter({'lay': 1}),\n",
              "              ('cat', 'lay'): Counter({'on': 1}),\n",
              "              ('lay', 'on'): Counter({'the': 1}),\n",
              "              ('on', 'the'): Counter({'bed': 1})}))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "m12 = collections.defaultdict(collections.Counter)\n",
        "for k in m1:\n",
        "    m12[k] += m1[k]\n",
        "for k in m2:\n",
        "    m12[k] += m2[k]\n",
        "m12, m1, m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L7DtbIqT_DE"
      },
      "source": [
        "We can wrap that up in a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LxF9beZ6T_DF"
      },
      "outputs": [],
      "source": [
        "def merge_models(model1, model2):\n",
        "    merged = collections.defaultdict(collections.Counter)\n",
        "    for k in model1:\n",
        "        merged[k] += model1[k]\n",
        "    for k in model2:\n",
        "        merged[k] += model2[k]\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyZ6bVMsT_DF"
      },
      "source": [
        "## Generating merged-model text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3g3Txu4T_DF"
      },
      "source": [
        "Let's combine some models and generate text with them. What does this generated text read like? What style is it in?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "uBazfR2JT_DF",
        "outputId": "01a3f6aa-85d6-44a9-8566-f2b448b2421c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "to me now. She is on her road somewhere, I dare say; but since he had had, it is possible-- that it still rolled in like muffled drums with a wild swell of voices added to them. He moved a chair with his foot. Not a word, especially to Miss Darcy. Say that urgent business calls us home immediately. Conceal the unhappy truth as long as they could be prevailed on to think as they thought, and after a time exclaimed, \" To treat in such a case .\" \" Let me interrupt you for a moment. I cannot read what I have not been greatly considered? We have lost many privileges; a new philosophy has become the mode; and the consciousness of this was another reason for his resolving to follow us. There is terror in the carriage to Meryton. And so, my dear, you must indeed go and see her ?\" The answers to this question, she could scarcely determine. After sitting in this manner, and too often. Your life is in your own hands. Quick! Call assistance !\" \" You cannot do your friend a better service .\" The Doctor, in a manner so little agreeable to her to be a clergyman. The business was therefore soon settled. He resigned all claim to it by extravagance, imprudence, in short, I have reason to think my opinions not entirely unalterable, they are not gone to Scotland. Colonel Forster gives us reason to expect an addition to our family, should have such an opportunity of having a companion. Lydia was occasionally a visitor there, when her husband was no longer an object. She could think only of what will make me happy, your affection, and secure of her relations' consent, there was a young' un and a straight made' un .\" Having smoked his pipe out, and then got shot dead himself by the other two was like; and each of them deep in thought. Elizabeth was surprised, but agreed to it immediately. Miss Bingley saw, or suspected, enough to be jealous; and her features are not at all old, who danced in public on the slightest provocation. Yet, a doubt lurks in my mind, Miss Pross was the family's devoted friend; Miss Pross knew full well that Madame Defarge was a stout woman of about his own age, or rather younger, and that he was very busy. \" Doctor Manette at home ?\" Expected home. \" I think so, too .\" \" If you were to give it up as soon as he could from such disgraceful companions. That he had his reasons for this"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "two_cities_pride_model = merge_models(two_cities_model, pride_model)\n",
        "pprint(generate_text(two_cities_pride_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "TxwZ-FkST_DF",
        "outputId": "01d4ef02-4e4d-4712-a198-c55882a9d470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "has a wall with his finger dipped in muddy wine- lees-- BLOOD. The time will come, the time will not be long delayed; your father remains at his old place in the midst of whom was the wood- sawyer's house, the name of his mother's family on Mt Parnassus. You and my mother had sent me to Autolycus, my mother's father, to receive the presents which when he was out shepherding, so we hurried on as fast as they could, whereon they set off, and I know better-- I shall never feel her weight .\" He carried her lightly to the door, with an inquiring look, and evidently understood it. \" You think so ?\" said Mr Cruncher. \" No, Mr Carton, if the poor lady had suffered so intensely from this cause before her little child was born, and the sheep lamb down three times a year. It would seem like flight, my darling! More than that; Monsieur Manette is, for it is decreed that you shall not know that, if the poor lady had suffered so intensely before her little child was born --\" \" The little child was born, and the coach was immediately filled with eight inside and a dozen out, while as for her complexion it was whiter than sawn ivory. When Minerva had done all this she showed no consideration for the sorrows of your mistress by crying in this way? my mind is set on marrying again, let her go as the wind and weather, better than I had done this I hid it under dung, which was very difficult. A numerous medley of men and women in Saint Antoine's bosom. CHAPTER XXIII. Fire Rises There was a murmur of commiseration as Charles Darnay crossed the room to a grated door where the witnesses go in, and they spoke very low, and I have no remedy .\" With this he left them to the suitors, that he looked quite stupidly at Mr Stryver shouldering him towards the door. \" Help, Gabelle! Help, every one !\" The tocsin rang impatiently, but other help ( if that could have been holden. \" If you expect me to befriend you when you have heard my story. Give me my work .\" Receiving no answer, but turned away to Erebus and to the other ghosts flit about aimlessly.' \" I listened to the echoes in the corner all through this space of time. Though days and nights long, which, to my surprise, seems to trouble you too much, aside. I declare to you, weep for it, weep for it, not she. Understand, then, and if your"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "two_cities_odyssey_model = merge_models(two_cities_model, odyssey_model)\n",
        "pprint(generate_text(two_cities_odyssey_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0a9-9sesT_DF",
        "outputId": "674e1de3-5850-4dd5-e142-8d8dddb145ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\" How could you begin ?\" said she. \" Wickham so very bad! It is admirable !\" Elizabeth tried to join in it; and, after a few moments. It was by Merlin's counsel. Then the queen, I fear not greatly to tell his name. Truly, said she, and I ought to have; and whatsomever ye spend ye shall have sufficiently, more than he was to begin his journey too early on the morrow there was an horrible lion kept in a strong castle here, fast by the sea, I will well, said King Mark, and there she made him such countenance that his heart well- nigh a twelvemonth throughout all England, and have no design of marrying, do not think Georgiana Darcy has her equal for beauty, elegance, and accomplishments; and the agreeable manner in which he immediately fell into conversation, though it could not dwell long on her spirits; and, after half an hour's visit, had revived. On Tuesday there was a great river, and the very great disadvantage to us all, which must arise from the public notice of Lydia's unguarded and imprudent manner, nay, my lord, ye are greatly to blame that ye saved not this lady's life. Madam, said Sir Launcelot. CHAPTER VIII. How the said knight came again the next night and was beheaded again, and more would he have been we should never have returned, wherefore I prove her false to God and to you no great hurt nor loss. And the eleven kings, and Dame Guenever, and she had seen at Rosings, in the whole course of my life .\" This roused a general astonishment; and he looked at her. She had heard nothing of Lady Catherine herself says that, in this field to keep the lords together and the commons, for the recovery of his daughter. \" He is just what a young- man ought to be done. How is such a presumption! And, besides, it was such a pity that great ladies in general. I cannot misunderstand you, but that I must needs, I will take care of myself, and therewithal he gave a great yawn and said, My lord the king, that ye shall most trust to of any man I know but ye overmatch him, and the King of Egypt and of Ethiopia, which were Sir Lancelot and Sir Tristram in the country at all is a most eligible match for their daughter, to whom he spoke, he left me goodly and hath staunched my blood. And so they washed and went to bed, from Colonel Forster, to inform him, at first,"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "arthur_pride_model = merge_models(arthur_model, pride_model)\n",
        "pprint(generate_text(arthur_pride_model, max_length=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7NCqKAjT_DF"
      },
      "source": [
        "You'll probably see the text in different styles. The combination of _Pride and Prejudice_ and _A Tale of Two Cities_ isn't too jarring, as the source texts are similar in style. But the combination of a ninteenth-century novel and an ancient epic poem is likely to have a disconcerting effect. You should be able to see a sentence or two that came from one source, then it switches to a sentence or two from the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-Sgg09T_DF"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHc6nYF_T_DF"
      },
      "source": [
        "You've seen a couple of things in this activity.\n",
        "\n",
        "First, you should have the idea of what a _language model_ is. It's nothing clever, just some data derived from source text that describes what's \"normal\" or \"expected\" from that source. The \"large language models\" of ChatGPT and similar work in a different way, and are much larger than this, but are doing essentially the same thing.\n",
        "\n",
        "Second, you've seen how \"style\" is something that can be encoded in a langauge model quite easily, and replicated fairly well.\n",
        "\n",
        "Third, you've seen how even randomly generated output can appear somewhat plausible if we don't engage our critical faculties enough to really pay attention to the output we see. Don't believe the hype around the current crop of AI systems!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAiCH38rT_DF"
      },
      "source": [
        "# Acknowledgements\n",
        "\n",
        "All the source texts used here come from [Project Gutenberg](https://www.gutenberg.org/), an online source of public domain works, with [certain permissions and conditions attached](https://www.gutenberg.org/policy/permission.html) . I've modified the books slightly from the versions available there, to remove the legal licence boilerplate and convert some characters to ASCII equivalents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GuJ-BySDT_DG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}